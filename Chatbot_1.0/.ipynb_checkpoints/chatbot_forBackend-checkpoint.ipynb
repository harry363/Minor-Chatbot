{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _____TF-IDF libraries_____\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# _____helper Libraries_____\n",
    "import pickle\n",
    "import csv\n",
    "import json\n",
    "import timeit\n",
    "import random\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(userQuery):\n",
    "    ques = []\n",
    "    temp = clean_text(userQuery.lower())\n",
    "    for token in temp.split(' '):\n",
    "        t = str(token)\n",
    "        if nlp.vocab[t].is_stop:\n",
    "            pass\n",
    "    #         print('no')\n",
    "        else:\n",
    "            ques.append(t)\n",
    "    ques_list = ' '.join(ques)\n",
    "\n",
    "    ques2 = []\n",
    "    temp2 = nlp(ques_list)\n",
    "    for token2 in temp2:\n",
    "        \n",
    "        ques2.append(token2.lemma_)\n",
    "    ques_list = ' '.join(ques2)\n",
    "    return ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_cb_primary(test_set_sentence, minimum_score , json_file_path , tfidf_vectorizer_pikle_path ,tfidf_matrix_train_pikle_path):\n",
    "    test_set = (test_set_sentence, \"\")\n",
    "\n",
    "    try:\n",
    "        f = open(tfidf_vectorizer_pikle_path, 'rb')\n",
    "        tfidf_vectorizer = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "        f = open(tfidf_matrix_train_pikle_path, 'rb')\n",
    "        tfidf_matrix_train = pickle.load(f)\n",
    "        f.close()\n",
    "    except:\n",
    "        print(\"NO TRAINING\")\n",
    "        \n",
    "    tfidf_matrix_test = tfidf_vectorizer.transform(test_set)\n",
    "\n",
    "    cosine = cosine_similarity(tfidf_matrix_test, tfidf_matrix_train)\n",
    "\n",
    "    cosine = np.delete(cosine, 0)\n",
    "    max = cosine.max()\n",
    "    response_index = 0\n",
    "    if (max > minimum_score):\n",
    "        new_max = max - 0.01\n",
    "        list = np.where(cosine > new_max)\n",
    "        response_index = random.choice(list[0])\n",
    "    else :\n",
    "        return \"Apologies, I can't understand. My developers haven't trained me much! :(\" , 0\n",
    "           \n",
    "\n",
    "    j = 0\n",
    "\n",
    "    with open(json_file_path, \"r\") as sentences_file:\n",
    "        reader = json.load(sentences_file)\n",
    "        for row in reader:\n",
    "            j += 1  # we begin with 1 not 0 &    j is initialized by 0\n",
    "            if j == response_index:\n",
    "                return row[\"response\"], max\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_chats(query):\n",
    "    minimum_score = 0.7\n",
    "    file = \"data.json\"\n",
    "    tfidf_vectorizer_pikle_path = \"previous_tfidf_vectorizer.pickle\"\n",
    "    tfidf_matrix_train_path = \"previous_tfidf_matrix_train.pickle\"\n",
    "    query_response, score = talk_to_cb_primary(query , minimum_score , file , tfidf_vectorizer_pikle_path , tfidf_matrix_train_path)\n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
